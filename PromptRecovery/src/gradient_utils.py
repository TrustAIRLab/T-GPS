import json
import re
import nltk
import concurrent.futures
import pandas as pd
import numpy as np

from openai import OpenAI
from abc import ABC, abstractmethod
from collections import defaultdict
from tqdm import tqdm
from nltk.tokenize import word_tokenize
from bert_score import score
from nltk.translate.meteor_score import meteor_score
from vllm import LLM
from sklearn.metrics.pairwise import cosine_similarity
from collections import OrderedDict

nltk.download('punkt')
nltk.download('wordnet')

OPENAI_API_KEY = 'your_openai_api_key_here'  # Replace with your OpenAI API key
client = OpenAI(api_key=OPENAI_API_KEY)

# Set embedding model
embedding_model = LLM(model="llm_models/intfloat/e5-mistral-7b-instruct")

def save_dict(dict, filename='target_prompt2recovery_prompt.json'):
    with open(filename, 'w') as file:
        json.dump(dict, file, indent=4)   

def save_tuple_dict(input_dict, filename):
    # Convert tuple keys to string
    converted_dict = {str(key): value for key, value in input_dict.items()}

    # Save the converted dictionary to a file
    with open(filename, 'w') as file:
        json.dump(converted_dict, file, indent=4)

def load_tuple_dict(filename):
    # Load the dictionary from the file
    with open(filename, 'r') as file:
        loaded_dict = json.load(file)
    
    # Convert string keys back to tuple keys
    converted_dict = {eval(key): value for key, value in loaded_dict.items()}

    return converted_dict
def load_json_file(file_path):
    with open(file_path, 'r') as file:
        data = json.load(file)
    return data

def load_responses_from_file(filename):
    with open(filename, 'r') as f:
        data = json.load(f)
    return data

def call_openai_api(prompt, model="gpt-3.5-turbo", temperature=0.7, seed=42, max_retry=3):
    attempts = 0
    while attempts < max_retry:
        try:
            completion = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                seed=seed
            )
            response = completion.choices[0].message.content
            return response
        except Exception as e:
            print(f"Attempt {attempts + 1} failed: {e}")
            attempts += 1
    return None
# gpt-4-turbo-2024-04-09
def single_get_draft_prompt(target_prompt, target_output, model='gpt-3.5-turbo'):
    full_prompt = f"What prompt would lead to this output: '{target_output}'?"
    draft_prompt = call_openai_api(full_prompt, model)
    if draft_prompt == None:
        return None
    else:
        return target_prompt, target_output, draft_prompt
    
def multi_get_draft_prompts(target_outputs, model='gpt-3.5-turbo'):
    target_output2draft_prompts = OrderedDict()
    with concurrent.futures.ThreadPoolExecutor(max_workers=32) as executor:
    # Map the API call function to each prompt
        futures = [executor.submit(single_get_draft_prompt, target_prompt, target_output, model) for target_prompt, target_output in target_outputs.items()]
        with tqdm(...) as pbar:
            for future in tqdm(futures):
                target_prompt, target_output, draft_prompt = future.result()
                target_output2draft_prompts[(target_prompt,target_output)] = draft_prompt   
    return target_output2draft_prompts

def evaluate_candidate_prompt(candidate_prompt, model="gpt-3.5-turbo"):
        # Given the candidate_prompt, get the output
        current_output = call_openai_api(candidate_prompt, model)
        if current_output == None:
            return None
        else:
            return current_output
        
def get_gradients(candidate_prompt, current_output, target_output,  model='gpt-3.5-turbo', num_feedbacks=5, n=1):
    """Get 'gradients' for a prompt based on the difference between target_output and current_output."""
    gradient_prompt = f"""
    I need help improving a prompt to better match a desired output. Below, I provide the desired output, the current prompt, and the actual output generated by the current prompt.

    Desired Target Output:
    "{target_output}"

    Current Prompt:
    "{candidate_prompt}"

    Output Generated by Current Prompt:
    "{current_output}"

    Please analyze the discrepancy between the 'Desired Target Output' and the 'Output Generated by Current Prompt'. Provide {num_feedbacks} specific reasons why the current prompt may not be producing the desired output. Format each reason as follows:

    <START> Reason {n}: [Insert Reason Here] <END>

    Ensure each reason is concise and directly relates to the differences observed. Increment the number for each reason.
    """
    gradient_prompt = '\n'.join([line.lstrip() for line in gradient_prompt.split('\n')])

    feedbacks = call_openai_api(gradient_prompt, model)
    if feedbacks == None:
        return None
    else:           
        # Compile a regular expression to find all text between <START> and <END>
        pattern = re.compile(r'Reason \d+: (.*?)(?=\n|$)')
        # Find all matches in the input string
        reasons = pattern.findall(feedbacks)
        return reasons   
def apply_gradient(candidate_prompt, current_output, target_output,  gradient,  model='gpt-3.5-turbo', steps_per_gradient=1):
        """Apply 'gradients' for a prompt based on the difference between target_output and current_output."""
        transformation_prompt = f"""
        I am seeking assistance to refine a prompt that will more closely align with a desired output. Below are details including the desired output, the current prompt, and the output that the current prompt generated. 

        Desired Target Output:
        "{target_output}"

        Current Prompt:
        "{candidate_prompt}"

        Output Generated by Current Prompt:
        "{current_output}"

        The main reasons why the current prompt is not producing the desired output: {gradient}

        Based on this analysis, please create {steps_per_gradient} improved versions of the prompt. Each new prompt should be clearly delineated with <START> and <END> tags.

        Please list the {steps_per_gradient} new prompts below:
        """
        transformation_prompt = '\n'.join([line.lstrip() for line in transformation_prompt.split('\n')])
        new_prompts_string = call_openai_api(transformation_prompt, model)
        if new_prompts_string == None:
            return None
        else:        
            # Compile a regular expression to find all text between <START> and <END>
            pattern = re.compile(r'<START>(.*?)<END>', re.S)

            # Find all matches in the input string
            new_prompts = pattern.findall(new_prompts_string)
        logs = '**Gradients**: \n' + f'- {gradient}\n'
        return new_prompts, logs

def expand_candidate(candidate_prompt, target_output, model='gpt-3.5-turbo'):
    new_candidate_prompts = []
    logs = ''
    logs += f'**Evaluating Prompt**: {candidate_prompt}\n'
    current_output = evaluate_candidate_prompt(candidate_prompt)
    if current_output == None:
        logs += f'**Evaluating Prompt**: None\n We skip this candidate {candidate_prompt}!! \n'
        return None, logs
    logs += f'**Generated Output**: {current_output}\n'

    gradients = get_gradients(candidate_prompt, current_output, target_output, model=model)
    if gradients == None or (len(gradients) == 0):
        logs += f'**Gradients**: None\n We skip this candidate {candidate_prompt}!! \n'
        return None, logs 
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(apply_gradient, candidate_prompt, current_output, target_output, gradient, model=model, steps_per_gradient=1) for gradient in gradients]
        for future in futures:
            new_prompts, new_log = future.result()
            logs += new_log
            if new_prompts != None and (len(new_prompts) > 0):
                for new_prompt in new_prompts:
                    logs += f'- **Generated New Prompt**: {new_prompt}\n'
                    new_candidate_prompts.append(new_prompt)
    return new_candidate_prompts, logs


def expand_candidates(target_prompt, candidate_prompts, target_output, model='gpt-3.5-turbo'):

    new_candidate_prompts = []
    logs = ''
    # num_workers = 4
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(expand_candidate, candidate_prompt, target_output, model=model) for candidate_prompt in candidate_prompts]
        for future in futures:
            new_prompts, new_log = future.result()
            logs += new_log
            if new_prompts != None and (len(new_prompts) > 0):
                for new_prompt in new_prompts:
                    new_candidate_prompts.append(new_prompt)

    return target_prompt, new_candidate_prompts, logs

def evaluate_candidate_prompt2(candidate_prompt, model="gpt-3.5-turbo"):
        # Given the candidate_prompt, get the output
        current_output = call_openai_api(candidate_prompt, model)
        if current_output == None:
            return candidate_prompt, None
        else:
            return candidate_prompt, current_output
        
def evaluate_candidate_prompts(target_prompt, candidate_prompts):
    target_candidate_prompts2outputs = {}
    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
        futures = [executor.submit(evaluate_candidate_prompt2, candidate_prompt) for candidate_prompt in candidate_prompts]
        for future in futures:
            candidate_prompt, output = future.result()
            if output==None:
                target_candidate_prompts2outputs[(target_prompt,candidate_prompt)] = ''
            else:
                target_candidate_prompts2outputs[(target_prompt,candidate_prompt)] = output
    
    return target_candidate_prompts2outputs


def score_new_candidates(target_prompts, target_outputs, target_prompts2new_candidates):
    # outputs are outputs generated by new_candidates
    # reference_outputs are target_outputs
    combined_target_candidate_prompts2outputs = {}
    new_order_target_candidate_prompts = []
    outputs = []
    reference_outputs = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=32) as executor:
        futures = [executor.submit(evaluate_candidate_prompts, target_prompt, target_prompts2new_candidates[target_prompt]) for target_prompt in target_prompts]
        for future in futures:
            target_candidate_prompts2outputs = future.result()
            for target_candidate_prompt, output in target_candidate_prompts2outputs.items():
                combined_target_candidate_prompts2outputs[target_candidate_prompt] = output
    
    for target_candidate_prompt, output in combined_target_candidate_prompts2outputs.items():
        new_order_target_candidate_prompts.append(target_candidate_prompt)
        outputs.append(output)
        reference_outputs.append(target_outputs[target_candidate_prompt[0]])
    
    # Compute Bert Score:
    P, R, F1 = score(outputs, reference_outputs, lang="en", verbose=True, device="cuda:0")
    F1 = F1.cpu().numpy().tolist()
    assert len(F1) == len(new_order_target_candidate_prompts), "F1 and candidate_prompts List lengths are not equal"
    bert_scores = F1

    # Compute Meteor:
    meteor_scores = []
    for reference_output, output in zip(reference_outputs, outputs):
        reference_tokens = word_tokenize(reference_output)
        output_tokens = word_tokenize(output)
        meteor_scores.append(meteor_score([reference_tokens], output_tokens))

    # Compute Semantic Score
    raw_output_embeddings = embedding_model.encode(outputs)
    raw_referenced_output_embeddings = embedding_model.encode(reference_outputs)
    output_embeddings = [raw_output_embedding.outputs.embedding for raw_output_embedding in raw_output_embeddings]
    referenced_output_embeddings = [raw_routput_embedding.outputs.embedding for raw_routput_embedding in raw_referenced_output_embeddings]

    cosine_similarities = cosine_similarity(output_embeddings, referenced_output_embeddings)
    llm_scores = cosine_similarities.diagonal().tolist()

    # Stack arrays vertically to form a matrix where each row represents a candidate
    scores_matrix = np.array([bert_scores,  meteor_scores, llm_scores]).T

    # Normalization of scores to a 0-1 scale
    min_vals = scores_matrix.min(axis=0)
    max_vals = scores_matrix.max(axis=0)
    scores_normalized = (scores_matrix - min_vals) / (max_vals - min_vals)

    # Weights for each metric
    weights = np.array([1/3, 1/3, 1/3])

    # Compute weighted scores and convert the result to a list
    weighted_scores_list = np.dot(scores_normalized, weights).tolist()

    assert len(weighted_scores_list) == len(new_order_target_candidate_prompts), "F1 and candidate_prompts List lengths are not equal"
    out_scores = defaultdict(dict)
    for i in range(len(weighted_scores_list)):
        out_scores[new_order_target_candidate_prompts[i][0]][new_order_target_candidate_prompts[i][1]] = weighted_scores_list[i]

    return out_scores